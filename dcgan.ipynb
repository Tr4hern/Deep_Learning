{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to the kaggle : https://www.kaggle.com/kvpratama/pokemon-images-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The aim of the project is to create a new pokemon based on the \"real\" ones (those created by Nintendo)\n",
    "##### To do that, we will have to use a GAN (Generate Adversarial Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch import nn, optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize : Resize the image to the given size (as a parameter)\n",
    "# CenterCrop : Crop the image at the center, the parameter is the size of the crop\n",
    "# ToTensor : Convert an Image (or PIL Image) to a numpy.array\n",
    "# Normalize : Normalize a tensor using (mean, stand deviation) as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory(directory):\n",
    "\n",
    "    if(os.path.exists(directory)):\n",
    "        shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_pokemon(dir_from, dir_to):\n",
    "\n",
    "    for file in os.listdir(dir_from):\n",
    "\n",
    "        original =  dir_from + file\n",
    "        target =  dir_to + file\n",
    "        \n",
    "        shutil.copy(original, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_pokemon(directory):\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "\n",
    "        file = file.replace('.png', '')\n",
    "\n",
    "        image = cv2.imread(directory + file + '.png')\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        cv2.imwrite(directory + file + '_flip.png', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_pokemon(directory):\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "\n",
    "        file = file.replace('.png', '')\n",
    "\n",
    "        original =  directory + file + '.png'\n",
    "        target =  directory + file + '_duplicate.png'\n",
    "        \n",
    "        shutil.copy(original, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_enough_pictures(directory):\n",
    "\n",
    "    p, d, files = next(os.walk(directory + '/pokemon/'))\n",
    "    number_of_files = len(files)\n",
    "\n",
    "    if(number_of_files >= 400):\n",
    "        directory_changed = False\n",
    "        path = directory\n",
    "        \n",
    "    else:\n",
    "        directory_changed = True\n",
    "        path = 'to_delete/'\n",
    "        path_2 = 'pokemon/'\n",
    "        paths = path + path_2\n",
    "\n",
    "        delete_directory(path)\n",
    "        os.mkdir(path)\n",
    "        os.mkdir(path + path_2)\n",
    "        copy_pokemon(directory + '/pokemon/', paths)\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        p, d, fs = next(os.walk(directory + '/pokemon/'))\n",
    "        nb_of_files = len(fs)\n",
    "\n",
    "        while(nb_of_files <= 400):\n",
    "\n",
    "\n",
    "            if(i%2 == 0):\n",
    "                flip_pokemon(paths)\n",
    "            \n",
    "            else:\n",
    "                duplicate_pokemon(paths)\n",
    "            \n",
    "            i = i + 1\n",
    "\n",
    "            p, d, fs = next(os.walk(paths))\n",
    "            nb_of_files = len(fs)\n",
    "\n",
    "\n",
    "    return path, directory_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainloader(path, size, batch):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.ImageFolder(root = path, transform = transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch, shuffle = True, num_workers = 2)\n",
    "\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing some Pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_graph(images, means, std):\n",
    "    \n",
    "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
    "    std = torch.tensor(std).reshape(1, 3, 1, 1)\n",
    "    return (images * std + means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some_pokemon(dataset, nb_pokemon, nb_rows):\n",
    "    \n",
    "    for images, labels in dataset: \n",
    "        fig, ax = plt.subplots(figsize = (15, 15))\n",
    "        reshaped =  reshape_for_graph(images, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ax.imshow(make_grid(reshaped[:nb_pokemon], nrow = nb_rows).permute(1, 2, 0).clamp(0, 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the GAN (Generative Adversarial Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN use two sub-models to work : a Generator and a Discriminator\n",
    "#### Basically, the Generator has to generate fake pictures while the Discriminator has to determine if a picture if a real one or a fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The aim of the Generator can be summed up with this picture :\n",
    "![Generator Model (in folder \"images\")](images/generator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The aim of the Discriminator can be summed up with this picture :\n",
    "![Discriminator Model (in folder \"images\")](images/discriminator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    \n",
    "    generator = Generator()\n",
    "    generator.to('cpu')\n",
    "    generator.apply(init_weights)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    \n",
    "    discriminator = Discriminator()\n",
    "    discriminator.to('cpu')\n",
    "    discriminator.apply(init_weights)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We start the machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning(trainloader, generator, discriminator, epochs):\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    fixed_noise = torch.randn(64, 100, 1, 1, device = 'cpu')\n",
    "\n",
    "    real_label = 1.\n",
    "    fake_label = 0.\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr = 0.0001, betas=(0.5, 0.999))\n",
    "    optimizer_dis = optim.Adam(discriminator.parameters(), lr = 0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    images = []\n",
    "    gen_losses, dis_losses = [], []\n",
    "    total = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        gen_loss, dis_loss = 0.0, 0.0\n",
    "        D_x, D_G_z1, D_G_z2 = 0.0, 0.0, 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            real_cpu = data[0].to('cpu')\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype = torch.float, device = 'cpu')\n",
    "            output = discriminator(real_cpu).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x += output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, 100, 1, 1, device = 'cpu')\n",
    "            fake = generator(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = discriminator(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 += output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizer_dis.step()\n",
    "\n",
    "\n",
    "            generator.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = discriminator(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 += output.mean().item()\n",
    "            optimizer_gen.step()\n",
    "            \n",
    "\n",
    "            gen_loss += errG.item()\n",
    "            dis_loss += errD.item()\n",
    "            total += b_size\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # END OF THE FOR LOOP IN THE FOR LOOP\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        avg_g_loss = gen_loss / total\n",
    "        gen_losses.append(avg_g_loss)\n",
    "        avg_d_loss = dis_loss / total\n",
    "        dis_losses.append(avg_d_loss)\n",
    "        \n",
    "        avg_D_x = D_x / len(trainloader)\n",
    "        avg_D_G_z1 = D_G_z1 / len(trainloader)\n",
    "        avg_D_G_z2 = D_G_z2 / len(trainloader)    \n",
    "\n",
    "        print('Epoch: {} / {} \\tDiscriminator Loss: {:.6f} \\tGenerator Loss: {:.6f}'.format(\n",
    "            epoch + 1,\n",
    "            epochs,\n",
    "            avg_d_loss,\n",
    "            avg_g_loss\n",
    "        ))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = generator(fixed_noise).detach().cpu()\n",
    "        images.append(make_grid(reshape_for_graph(fake, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), padding = 2, normalize=True))\n",
    "\n",
    "    losses = [gen_losses, dis_losses]\n",
    "\n",
    "\n",
    "    return generator, discriminator, images, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_losses(gen_losses, dis_losses):\n",
    "    \n",
    "    plt.figure(figsize = (15, 15))\n",
    "    plt.plot(gen_losses, label = \"Generator\")\n",
    "    plt.plot(dis_losses, label = \"Discriminator\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the models when we are satisfied with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gen(generator, path):\n",
    "    \n",
    "    torch.save(generator.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dis(discriminator, path):\n",
    "    \n",
    "    torch.save(discriminator.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gen(path, type):\n",
    "    \n",
    "    model_load_G = Generator()\n",
    "    model_load_G.load_state_dict(torch.load(path + type + '.pth'))\n",
    "    \n",
    "    return model_load_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dis(path, type):\n",
    "    \n",
    "    model_load_D = Discriminator()\n",
    "    model_load_D.load_state_dict(torch.load(path + type + '.pth'))\n",
    "    \n",
    "    return model_load_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fakemons(generator):\n",
    "    fixed_noise = torch.randn(64, 100, 1, 1, device = 'cpu')\n",
    "    images = []\n",
    "\n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            fake = generator(fixed_noise).detach().cpu()\n",
    "        images.append(torchvision.utils.make_grid(reshape_for_graph(fake, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), padding = 2, normalize = True))\n",
    "    \n",
    "    return images"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b4e6c2540be57ec5977bbdbc1154b133ad304eea54dedcf1573a26853d1cf25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
